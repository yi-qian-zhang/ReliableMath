#!/usr/bin/env python3
"""
MIP (Missing Information Problem) Construction Pipeline - 2 Steps Version with Sampling
ä½¿ç”¨ vLLM sampling åŠŸèƒ½ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªå€™é€‰ç­”æ¡ˆï¼ŒåŠ é€ŸéªŒè¯è¿‡ç¨‹
"""

import os
import json
import time
import logging
import argparse
from openai import OpenAI
import random
from tqdm import tqdm
import tiktoken
import glob
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

parser = argparse.ArgumentParser(description="MIP Dataset Construction - 2 Steps with Sampling")
parser.add_argument("--model", default="gpt-4o", help="Model for extraction/rewrite")
parser.add_argument("--verify_model", default="deepseek-r1-distill-qwen-7b", help="Model for verification")
parser.add_argument("--judge_model", default="gpt-4o-mini", help="Model for LLM-as-Judge")
parser.add_argument("--data_dir", default="data/solve", help="Input directory")
parser.add_argument("--output_dir", default="data/construct_mip_data", help="Output directory")
parser.add_argument("--prompt_dir", default="prompt/construct_mip_data", help="Prompt directory")
parser.add_argument("--dataset", default="polaris_easy_20", help="Dataset name")
parser.add_argument("--temperature", default=0.9, type=float, help="Temperature for verification")
parser.add_argument("--max_attempts", default=8, type=int, help="Max attempts for verification")
parser.add_argument("--threads", default=4, type=int, help="Number of parallel threads")
parser.add_argument("--test_mode", action='store_true', help="Test mode - process only first 5 items")
parser.add_argument("--force", action='store_true', help="Force reprocess all items")
args = parser.parse_args()

# Load API config
try:
    api_config_path = "data/api_keys.json"
    model_options = json.load(open(api_config_path, "r"))
except FileNotFoundError:
    logging.error(f"api_keys.json not found at {api_config_path}!")
    logging.error(f"Please make sure you run this script from ~/ReliableMath directory")
    exit(1)

# å…¨å±€é”ï¼Œç”¨äºä¿æŠ¤ JSONL æ–‡ä»¶å†™å…¥
jsonl_write_lock = threading.Lock()

# ============= Utility Functions =============

def read_json(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def write_json(filepath, data):
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def read_jsonl(filepath):
    data = []
    if not os.path.exists(filepath):
        return data
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except:
                    continue
    return data

def dump_jsonl(data, filepath, append=False):
    """çº¿ç¨‹å®‰å…¨çš„ JSONL å†™å…¥"""
    mode = 'a' if append else 'w'
    try:
        json_str = json.dumps(data, ensure_ascii=False)
    except:
        return False
    
    with jsonl_write_lock:
        with open(filepath, mode, encoding='utf-8') as f:
            f.write(json_str + '\n')
            f.flush()
    return True

def count_tokens(text, model_name="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model_name)
        return len(encoding.encode(text))
    except:
        return len(text) // 4

def record_tokens(data, model_type, prompt_tokens, completion_tokens):
    """
    æ ¹æ®æ¨¡å‹ç±»å‹è®°å½• token ä½¿ç”¨é‡
    
    å‚æ•°ï¼š
        data: æ•°æ®å­—å…¸
        model_type: "gpt-4o" / "gpt-4o-mini" / "local"
        prompt_tokens: è¾“å…¥ token æ•°
        completion_tokens: è¾“å‡º token æ•°
    """
    # åˆå§‹åŒ–å­—æ®µ
    if "gpt4o_prompt_lengths" not in data:
        data["gpt4o_prompt_lengths"] = []
        data["gpt4o_completion_lengths"] = []
    if "gpt4o_mini_prompt_lengths" not in data:
        data["gpt4o_mini_prompt_lengths"] = []
        data["gpt4o_mini_completion_lengths"] = []
    if "local_prompt_lengths" not in data:
        data["local_prompt_lengths"] = []
        data["local_completion_lengths"] = []
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è®°å½•
    if model_type == "gpt-4o":
        data["gpt4o_prompt_lengths"].append(prompt_tokens)
        data["gpt4o_completion_lengths"].append(completion_tokens)
    elif model_type == "gpt-4o-mini":
        data["gpt4o_mini_prompt_lengths"].append(prompt_tokens)
        data["gpt4o_mini_completion_lengths"].append(completion_tokens)
    elif model_type == "local":
        data["local_prompt_lengths"].append(prompt_tokens)
        data["local_completion_lengths"].append(completion_tokens)

# ============= API Functions =============

def get_response_openai(input_prompt, persona="", model=None, temperature=0.0):
    """
    è°ƒç”¨ OpenAI-compatible APIï¼ˆå•ä¸ªå“åº”ï¼‰
    
    è¿”å›ï¼š
        (response_text, prompt_tokens, completion_tokens, model_type)
        model_type: "gpt-4o" / "gpt-4o-mini" / "local"
    """
    if model is None:
        model = args.model
    
    if model not in model_options:
        logging.error(f"Model {model} not found")
        return "", 0, 0, "unknown"
    
    model_name, key, url = random.choice(model_options[model])
    client = OpenAI(api_key=key, base_url=url)
    
    messages = []
    if persona:
        messages.append({"role": "system", "content": persona})
    messages.append({"role": "user", "content": input_prompt})
    
    prompt_text = (persona + "\n" if persona else "") + input_prompt
    prompt_tokens = count_tokens(prompt_text, model_name)
    
    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    is_local_model = "localhost" in url or "127.0.0.1" in url
    
    if is_local_model:
        model_type = "local"
    elif "gpt-4o-mini" in model_name.lower():
        model_type = "gpt-4o-mini"
    elif "gpt-4o" in model_name.lower():
        model_type = "gpt-4o"
    else:
        model_type = "gpt-4o"
    
    max_retries = 5
    for attempt in range(max_retries):
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=8192,
                stream=False
            )
            
            response_text = completion.choices[0].message.content
            
            try:
                prompt_tokens = completion.usage.prompt_tokens
                completion_tokens = completion.usage.completion_tokens
            except:
                if is_local_model:
                    logging.debug(f"Local model: estimating tokens")
                completion_tokens = count_tokens(response_text, model_name)
            
            return response_text, prompt_tokens, completion_tokens, model_type
            
        except Exception as e:
            logging.warning(f'API call failed (attempt {attempt+1}/{max_retries}): {e}')
            if attempt < max_retries - 1:
                wait_time = 3 if is_local_model else 10
                time.sleep(wait_time * (attempt + 1))
    
    return "", 0, 0, model_type

def get_response_openai_with_sampling(input_prompt, persona="", model=None, temperature=0.0, n=1):
    """
    è°ƒç”¨ OpenAI-compatible APIï¼Œæ”¯æŒ samplingï¼ˆä¸€æ¬¡ç”Ÿæˆå¤šä¸ªå€™é€‰ç­”æ¡ˆï¼‰
    
    å‚æ•°ï¼š
        n: ç”Ÿæˆçš„å€™é€‰ç­”æ¡ˆæ•°é‡ï¼ˆé»˜è®¤ 1ï¼‰
    
    è¿”å›ï¼š
        {
            "candidates": [ç­”æ¡ˆ1, ç­”æ¡ˆ2, ..., ç­”æ¡ˆn],
            "prompt_tokens": xxx,
            "completion_tokens": xxx,
            "model_type": "gpt-4o" / "gpt-4o-mini" / "local"
        }
        æˆ–è€…å¤±è´¥æ—¶è¿”å› None
    """
    if model is None:
        model = args.model
    
    if model not in model_options:
        logging.error(f"Model {model} not found")
        return None
    
    model_name, key, url = random.choice(model_options[model])
    client = OpenAI(api_key=key, base_url=url)
    
    messages = []
    if persona:
        messages.append({"role": "system", "content": persona})
    messages.append({"role": "user", "content": input_prompt})
    
    prompt_text = (persona + "\n" if persona else "") + input_prompt
    prompt_tokens = count_tokens(prompt_text, model_name)
    
    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    is_local_model = "localhost" in url or "127.0.0.1" in url
    
    if is_local_model:
        model_type = "local"
    elif "gpt-4o-mini" in model_name.lower():
        model_type = "gpt-4o-mini"
    elif "gpt-4o" in model_name.lower():
        model_type = "gpt-4o"
    else:
        model_type = "gpt-4o"
    
    max_retries = 5
    for attempt in range(max_retries):
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                n=n,  # â† å…³é”®ï¼šä¸€æ¬¡ç”Ÿæˆ n ä¸ªå€™é€‰ç­”æ¡ˆ
                max_tokens=8192,
                stream=False
            )
            
            # æå–æ‰€æœ‰å€™é€‰ç­”æ¡ˆ
            candidates = [choice.message.content for choice in completion.choices]
            
            # è·å– token ç»Ÿè®¡
            try:
                prompt_tokens = completion.usage.prompt_tokens
                completion_tokens = completion.usage.completion_tokens
            except:
                if is_local_model:
                    logging.debug(f"Local model: estimating tokens")
                # ä¼°ç®—æ‰€æœ‰å€™é€‰çš„æ€» token
                completion_tokens = sum(count_tokens(text, model_name) for text in candidates)
            
            return {
                "candidates": candidates,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "model_type": model_type
            }
            
        except Exception as e:
            logging.warning(f'API call failed (attempt {attempt+1}/{max_retries}): {e}')
            if attempt < max_retries - 1:
                wait_time = 3 if is_local_model else 10
                time.sleep(wait_time * (attempt + 1))
    
    return None

def parse_json_response(response, fallback=None):
    """ç®€åŒ–çš„ JSON è§£æ"""
    try:
        start = response.find('[')
        end = response.rfind(']') + 1
        if start >= 0 and end > start:
            json_str = response[start:end]
            return json.loads(json_str)
        
        start = response.find('{')
        end = response.rfind('}') + 1
        if start >= 0 and end > start:
            json_str = response[start:end]
            return json.loads(json_str)
            
    except Exception as e:
        logging.error(f"JSON parsing failed: {e}")
        logging.error(f"Full response: {response}")
    
    return fallback if fallback is not None else {}

# ============= Answer Processing =============

def extract_answer_tag(response):
    """ä»å“åº”ä¸­æå–ç­”æ¡ˆï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
    try:
        # æ–¹æ³• 1: ä¼˜å…ˆæŸ¥æ‰¾ <answer> æ ‡ç­¾
        start = response.find('<answer>')
        end = response.find('</answer>')
        
        if start >= 0 and end > start:
            answer = response[start + 8:end].strip()
            if '\\boxed{' in answer:
                boxed_match = re.search(r'\\boxed\{([^}]+)\}', answer)
                if boxed_match:
                    return boxed_match.group(1).strip()
            return answer
        
        # æ–¹æ³• 2: æŸ¥æ‰¾ $\boxed{...}$ æˆ– \boxed{...} æ ¼å¼
        boxed_pattern = r'\$?\\boxed\{([^}]+)\}\$?'
        boxed_matches = re.findall(boxed_pattern, response)
        
        if boxed_matches:
            answer = boxed_matches[-1].strip()
            answer = answer.replace('$', '').strip()
            return answer
        
        # æ–¹æ³• 3: æŸ¥æ‰¾å¸¸è§çš„ç­”æ¡ˆæ ‡è®°
        answer_patterns = [
            r'[Ff]inal [Aa]nswer:?\s*(.+?)(?:\n|$)',
            r'[Tt]he answer is:?\s*(.+?)(?:\n|$)',
            r'[Aa]nswer:?\s*(.+?)(?:\n|$)',
        ]
        
        for pattern in answer_patterns:
            match = re.search(pattern, response)
            if match:
                answer = match.group(1).strip()
                if '\\boxed{' in answer:
                    boxed_match = re.search(r'\\boxed\{([^}]+)\}', answer)
                    if boxed_match:
                        return boxed_match.group(1).strip()
                return answer
        
        return None
        
    except Exception as e:
        logging.error(f"Failed to extract answer: {e}")
        return None

def judge_answer_equivalence(question, model_answer, ground_truth):
    """ä½¿ç”¨ LLM-as-Judge åˆ¤æ–­ç­”æ¡ˆç­‰ä»·æ€§"""
    prompt_path = os.path.join(args.prompt_dir, "judge_equivalence.txt")
    
    if not os.path.exists(prompt_path):
        logging.error(f"Judge prompt not found: {prompt_path}")
        return False, 0, 0, "unknown"
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    
    input_prompt = prompt_template.format(
        question=question,
        model_answer=model_answer,
        ground_truth=ground_truth
    )
    
    response, prompt_tokens, completion_tokens, model_type = get_response_openai(
        input_prompt,
        persona="You are an expert mathematical equivalence judge.",
        model=args.judge_model,
        temperature=0.0
    )
    
    response_lower = response.strip().lower()
    
    if 'true' in response_lower and 'false' not in response_lower:
        result = True
    elif response_lower == 'true':
        result = True
    else:
        result = False
    
    return result, prompt_tokens, completion_tokens, model_type

# ============= Step 1: Extract and Generate Variants =============

def extract_and_generate_variants(data):
    """Step 1: ä¸€æ¬¡æ€§æå–æ¡ä»¶å¹¶ç”Ÿæˆæ‰€æœ‰ç§»é™¤å˜ä½“"""
    prompt_path = os.path.join(args.prompt_dir, "extract_and_remove.txt")
    
    if not os.path.exists(prompt_path):
        logging.error(f"Prompt file not found: {prompt_path}")
        data["removal_variants"] = []
        return data
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    
    input_prompt = prompt_template.format(
        original_question=data["question"],
        ground_truth=data.get("ground_truth", "")
    )
    
    response, prompt_tokens, completion_tokens, model_type = get_response_openai(
        input_prompt,
        persona="You are an expert at analyzing and rewriting mathematical problems.",
        model=args.model,
        temperature=0.0
    )
    
    # è®°å½• token
    record_tokens(data, model_type, prompt_tokens, completion_tokens)
    
    # Parse response - æœŸæœ›å¾—åˆ°ä¸€ä¸ªå˜ä½“åˆ—è¡¨
    parsed = parse_json_response(response, {"variants": []})
    
    # å¤„ç†ä¸¤ç§å¯èƒ½çš„ JSON æ ¼å¼
    if isinstance(parsed, list):
        variants_data = parsed
    else:
        variants_data = parsed.get("variants", [])
    
    removal_variants = []
    
    for i, variant_data in enumerate(variants_data):
        # æ¸…ç† incomplete_question
        incomplete_question = variant_data.get("incomplete_question", "").strip()
        
        # Remove common prefixes
        for prefix in ["Rewritten Problem:", "Incomplete Problem:", "Problem:", "**Problem:**"]:
            if prefix in incomplete_question:
                incomplete_question = incomplete_question.split(prefix)[-1].strip()
        
        incomplete_question = incomplete_question.replace("**", "").strip()
        
        # Remove quotes if present
        if incomplete_question.startswith('"') and incomplete_question.endswith('"'):
            incomplete_question = incomplete_question[1:-1].strip()
        
        variant = {
            "variant_id": f"{data['id']}_remove_{i}",
            "removed_condition_index": i,
            "removed_condition": variant_data.get("removed_condition", ""),
            "remaining_conditions": variant_data.get("remaining_conditions", []),
            "incomplete_question": incomplete_question
        }
        
        removal_variants.append(variant)
    
    data["removal_variants"] = removal_variants
    
    logging.info(f"ID {data['id']}: Generated {len(removal_variants)} removal variants")
    
    return data

# ============= Step 2: Verify with Sampling =============

def verify_single_variant(data, variant, prompt_template, ground_truth):
    """éªŒè¯å•ä¸ªå˜ä½“ï¼ˆä½¿ç”¨ sampling ä¸€æ¬¡æ€§ç”Ÿæˆå¤šä¸ªå€™é€‰ç­”æ¡ˆï¼‰"""
    incomplete_question = variant["incomplete_question"]
    removed_condition = variant["removed_condition"]
    
    input_prompt = prompt_template.format(
        incomplete_question=incomplete_question,
        removed_condition=removed_condition
    )
    
    # ========== å…³é”®ä¿®æ”¹ï¼šä¸€æ¬¡ç”Ÿæˆ max_attempts ä¸ªå€™é€‰ç­”æ¡ˆ ==========
    response_data = get_response_openai_with_sampling(
        input_prompt,
        persona="You are an expert mathematical problem solver.",
        model=args.verify_model,
        temperature=args.temperature,
        n=args.max_attempts  # ä¸€æ¬¡ç”Ÿæˆ max_attempts ä¸ªç­”æ¡ˆ
    )
    
    if not response_data:
        # ç”Ÿæˆå¤±è´¥ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ
        logging.error(f"ID {variant['variant_id']}: Failed to generate candidates")
        variant["verification"] = {
            "total_attempts": 0,
            "success_at_attempt": None,
            "is_valid": False,
            "all_attempts": [],
            "ground_truth": ground_truth
        }
        return variant
    
    # è§£åŒ…æ•°æ®
    all_candidates = response_data["candidates"]  # max_attempts ä¸ªå€™é€‰ç­”æ¡ˆ
    prompt_tokens = response_data["prompt_tokens"]
    completion_tokens = response_data["completion_tokens"]
    model_type = response_data["model_type"]
    
    # è®°å½•ç”Ÿæˆçš„ tokenï¼ˆåªè®°å½•ä¸€æ¬¡ï¼‰
    record_tokens(data, model_type, prompt_tokens, completion_tokens)
    
    logging.info(f"ID {variant['variant_id']}: Generated {len(all_candidates)} candidates, checking...")
    
    # ========== ä¾æ¬¡æ£€æŸ¥æ¯ä¸ªå€™é€‰ç­”æ¡ˆ ==========
    all_attempts = []
    success_at_attempt = None
    is_valid = False
    
    for attempt_num, candidate_text in enumerate(all_candidates, start=1):
        # æå–ç­”æ¡ˆ
        model_answer = extract_answer_tag(candidate_text)
        
        # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
        if model_answer is None:
            is_correct = False
            judge_result = "no_answer_tag"
        else:
            is_correct, judge_prompt_tokens, judge_completion_tokens, judge_model_type = judge_answer_equivalence(
                incomplete_question + " [With condition: " + removed_condition + "]",
                model_answer,
                ground_truth
            )
            judge_result = "equivalent" if is_correct else "not_equivalent"
            
            # è®°å½• judge token
            record_tokens(data, judge_model_type, judge_prompt_tokens, judge_completion_tokens)
        
        # è®°å½•æœ¬æ¬¡å°è¯•
        attempt_record = {
            "attempt": attempt_num,
            "model_answer": model_answer if model_answer else "N/A",
            "judge_result": judge_result,
            "is_correct": is_correct
        }
        all_attempts.append(attempt_record)
        
        # å¦‚æœç­”å¯¹äº†ï¼Œç«‹å³åœæ­¢æ£€æŸ¥åç»­å€™é€‰
        if is_correct:
            success_at_attempt = attempt_num
            is_valid = True
            logging.info(f"ID {variant['variant_id']}: âœ“ VALID at candidate {attempt_num}/{args.max_attempts}")
            break
        else:
            logging.debug(f"ID {variant['variant_id']}: Candidate {attempt_num}/{args.max_attempts} failed")
    
    # å¦‚æœæ‰€æœ‰å€™é€‰éƒ½å¤±è´¥
    if not is_valid:
        logging.info(f"ID {variant['variant_id']}: âœ— INVALID - All {args.max_attempts} candidates failed")
    
    # ä¿å­˜éªŒè¯ç»“æœ
    variant["verification"] = {
        "total_attempts": len(all_attempts),
        "success_at_attempt": success_at_attempt,
        "is_valid": is_valid,
        "all_attempts": all_attempts,
        "ground_truth": ground_truth
    }
    
    return variant

def verify_incomplete_questions_with_sampling(data):
    """Step 2: éªŒè¯"ç¼ºçœé—®é¢˜ + ç§»é™¤çš„æ¡ä»¶"èƒ½å¦è§£å‡º ground_truthï¼ˆå¹¶è¡Œå¤„ç†å˜ä½“ï¼Œä½¿ç”¨ samplingï¼‰"""
    prompt_path = os.path.join(args.prompt_dir, "verify_with_condition.txt")
    
    if not os.path.exists(prompt_path):
        logging.error(f"Prompt file not found: {prompt_path}")
        return data
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    
    ground_truth = str(data.get("ground_truth", "")).strip()
    variants = data.get("removal_variants", [])
    
    if not variants:
        return data
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰å˜ä½“
    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        future_to_variant = {
            executor.submit(verify_single_variant, data, variant, prompt_template, ground_truth): variant
            for variant in variants
        }
        
        for future in as_completed(future_to_variant):
            try:
                verified_variant = future.result()
                # æ›´æ–° data ä¸­å¯¹åº”çš„ variant
                variant_id = verified_variant["variant_id"]
                for i, v in enumerate(data["removal_variants"]):
                    if v["variant_id"] == variant_id:
                        data["removal_variants"][i] = verified_variant
                        break
            except Exception as e:
                variant = future_to_variant[future]
                logging.error(f"Error verifying {variant['variant_id']}: {e}")
                import traceback
                traceback.print_exc()
    
    return data

# ============= Pipeline Functions =============

def process_with_jsonl_parallel(dataset, output_path, process_func, desc):
    """å¹¶è¡Œå¤„ç†æ•°æ®é›†"""
    total_len = len(dataset)
    jsonl_path = output_path.replace('.json', '.jsonl')
    
    existing_data = []
    if os.path.exists(jsonl_path):
        existing_data = read_jsonl(jsonl_path)
        if existing_data:
            saved_ids = {item['id'] for item in existing_data}
            dataset = [item for item in dataset if item['id'] not in saved_ids]
            logging.info(f"{desc}: Continuing from {len(existing_data)} items")
    elif os.path.exists(output_path):
        try:
            existing_data = read_json(output_path)
            saved_ids = {item['id'] for item in existing_data}
            dataset = [item for item in dataset if item['id'] not in saved_ids]
        except:
            pass
    
    if not dataset:
        logging.info(f"{desc}: All items processed")
        return True
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_data = {executor.submit(process_func, data): data for data in dataset}
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(dataset), desc=desc) as pbar:
            for future in as_completed(future_to_data):
                try:
                    processed_data = future.result()
                    if processed_data:
                        dump_jsonl(processed_data, jsonl_path, append=True)
                    pbar.update(1)
                except Exception as e:
                    data = future_to_data[future]
                    logging.error(f"Error processing {data.get('id', 'unknown')}: {e}")
                    import traceback
                    traceback.print_exc()
                    pbar.update(1)
    
    # åˆå¹¶æ•°æ®
    all_data = existing_data + read_jsonl(jsonl_path)[len(existing_data):]
    
    if all_data:
        # ========== æ–°å¢ï¼šæŒ‰ ID æ’åº ==========
        all_data.sort(key=lambda x: x.get('id', 0))
        # ===================================
        
        write_json(output_path, all_data)
        if os.path.exists(jsonl_path):
            os.remove(jsonl_path)
    
    return len(all_data) == total_len

def filter_valid_data(final_path):
    """ç­›é€‰æœ‰æ•ˆçš„ç¼ºçœé—®é¢˜"""
    dataset = read_json(final_path)
    valid_data = []
    
    # åˆ†åˆ«ç»Ÿè®¡ä¸‰ç±»æ¨¡å‹çš„ token
    total_gpt4o_prompt = sum(sum(d.get("gpt4o_prompt_lengths", [])) for d in dataset)
    total_gpt4o_completion = sum(sum(d.get("gpt4o_completion_lengths", [])) for d in dataset)
    
    total_gpt4o_mini_prompt = sum(sum(d.get("gpt4o_mini_prompt_lengths", [])) for d in dataset)
    total_gpt4o_mini_completion = sum(sum(d.get("gpt4o_mini_completion_lengths", [])) for d in dataset)
    
    total_local_prompt = sum(sum(d.get("local_prompt_lengths", [])) for d in dataset)
    total_local_completion = sum(sum(d.get("local_completion_lengths", [])) for d in dataset)
    
    total_original = len(dataset)
    total_variants = 0
    valid_variants = 0
    
    # ç»Ÿè®¡å°è¯•æ¬¡æ•°åˆ†å¸ƒ
    attempt_distribution = {}
    
    for data in dataset:
        for variant in data.get("removal_variants", []):
            total_variants += 1
            
            verification = variant.get("verification", {})
            
            # åªä¿ç•™æœ‰æ•ˆçš„ pairï¼ˆåŠ å›æ¡ä»¶åèƒ½è§£å‡º ground_truthï¼‰
            if verification.get("is_valid", False):
                success_attempt = verification.get("success_at_attempt", 0)
                attempt_distribution[success_attempt] = attempt_distribution.get(success_attempt, 0) + 1
                
                valid_item = {
                    "id": variant["variant_id"],
                    "data_source": data.get("data_source", ""),
                    "difficulty": data.get("difficulty", ""),
                    "transformation_type": "condition_removal",
                    "original_question": data["question"],
                    "ground_truth": data.get("ground_truth", ""),
                    "removed_condition": variant["removed_condition"],
                    "removed_condition_index": variant["removed_condition_index"],
                    "remaining_conditions": variant["remaining_conditions"],
                    "incomplete_question": variant["incomplete_question"],
                    "verification": verification,
                    "original_id": data["id"]
                }
                valid_data.append(valid_item)
                valid_variants += 1
    
    # ========== æ–°å¢ï¼šæŒ‰ ID æ’åºï¼ˆè€ƒè™‘ variant_id æ ¼å¼ï¼‰==========
    # variant_id æ ¼å¼ï¼š0_remove_0, 0_remove_1, 1_remove_0 ç­‰
    # æ’åºè§„åˆ™ï¼šå…ˆæŒ‰åŸå§‹ IDï¼Œå†æŒ‰ removed_condition_index
    valid_data.sort(key=lambda x: (x.get('original_id', 0), x.get('removed_condition_index', 0)))
    # ========================================================
    
    output_path = final_path.replace("_final.json", "_valid.json")
    write_json(output_path, valid_data)
    
    # Statistics
    print("\n" + "="*70)
    print("MISSING INFORMATION PROBLEM (MIP) DATASET STATISTICS")
    print("="*70)
    print(f"Original problems: {total_original}")
    print(f"\nTotal removal variants generated: {total_variants}")
    print(f"Valid removal variants (condition necessary): {valid_variants}")
    if total_variants > 0:
        print(f"Success rate: {valid_variants / total_variants * 100:.2f}%")
    
    print(f"\nAttempt Distribution (when successful):")
    for attempt in sorted(attempt_distribution.keys()):
        count = attempt_distribution[attempt]
        print(f"  Candidate {attempt}: {count} variants ({count/valid_variants*100:.1f}%)")
    
    # å•ä»·ï¼ˆæ¯ 1M tokensï¼‰
    gpt4o_prompt_rate = 2.5
    gpt4o_completion_rate = 10.0
    gpt4o_mini_prompt_rate = 0.15
    gpt4o_mini_completion_rate = 0.6

    # GPT-4o Token ç»Ÿè®¡
    print(f"\nğŸ’° GPT-4o Token Usage:")
    print(f"  Prompt: {total_gpt4o_prompt:,}")
    print(f"  Completion: {total_gpt4o_completion:,}")
    print(
        f"  Cost = {total_gpt4o_prompt}/1e6*{gpt4o_prompt_rate} "
        f"+ {total_gpt4o_completion}/1e6*{gpt4o_completion_rate} "
        f"= ${total_gpt4o_prompt/1e6*gpt4o_prompt_rate + total_gpt4o_completion/1e6*gpt4o_completion_rate:.6f}"
    )

    # GPT-4o-mini Token ç»Ÿè®¡
    print(f"\nğŸ’° GPT-4o-mini Token Usage:")
    print(f"  Prompt: {total_gpt4o_mini_prompt:,}")
    print(f"  Completion: {total_gpt4o_mini_completion:,}")
    print(
        f"  Cost = {total_gpt4o_mini_prompt}/1e6*{gpt4o_mini_prompt_rate} "
        f"+ {total_gpt4o_mini_completion}/1e6*{gpt4o_mini_completion_rate} "
        f"= ${total_gpt4o_mini_prompt/1e6*gpt4o_mini_prompt_rate + total_gpt4o_mini_completion/1e6*gpt4o_mini_completion_rate:.6f}"
    )
    
    # æœ¬åœ°æ¨¡å‹ Token ç»Ÿè®¡
    print(f"\nğŸ–¥ï¸  Local Model Token Usage:")
    print(f"  Prompt: {total_local_prompt:,}")
    print(f"  Completion: {total_local_completion:,}")
    
    print(f"\nOutput: {output_path}")
    print("="*70)

# ============= Main Workflow =============

def construction_workflow():
    # ç›´æ¥ä½¿ç”¨ args ä¸­çš„è·¯å¾„ï¼ˆç›¸å¯¹äº ~/ReliableMathï¼‰
    input_path = os.path.join(args.data_dir, f"{args.dataset}.json")
    output_dir = args.output_dir
    
    if not os.path.exists(input_path):
        logging.error(f"Input not found: {input_path}")
        logging.error(f"Current working directory: {os.getcwd()}")
        logging.error(f"Please make sure you run this script from ~/ReliableMath directory")
        return
    
    dataset = read_json(input_path)
    
    if args.test_mode:
        dataset = dataset[:5]
        logging.info("TEST MODE: First 5 items")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Force cleanup
    if args.force:
        logging.info("Force mode: Cleaning up existing intermediate files...")
        for pattern in [f"{args.dataset}_*.json", f"{args.dataset}_*.jsonl"]:
            for file in glob.glob(os.path.join(output_dir, pattern)):
                try:
                    os.remove(file)
                    logging.info(f"Removed: {file}")
                except Exception as e:
                    logging.warning(f"Could not remove {file}: {e}")
        logging.info("Cleanup completed.")
    
    print("="*70)
    print("MIP CONSTRUCTION PIPELINE - WITH SAMPLING OPTIMIZATION")
    print("="*70)
    print(f"Working directory: {os.getcwd()}")
    print(f"Input: {input_path}")
    print(f"Output: {output_dir}")
    print(f"Prompt: {args.prompt_dir}")
    print(f"Model (extract/rewrite): {args.model}")
    print(f"Model (verify): {args.verify_model}")
    print(f"Model (judge): {args.judge_model}")
    print(f"Temperature: {args.temperature}")
    print(f"Max attempts (sampling n): {args.max_attempts}")
    print(f"Parallel threads: {args.threads}")
    print(f"Items: {len(dataset)}")
    if args.force:
        print(f"Mode: FORCE (reprocessing all)")
    print("="*70)
    
    # Step 1: Extract and Generate Variants (å¹¶è¡Œ)
    print("\n[1/3] Extracting conditions and generating removal variants (parallel)")
    extract_path = os.path.join(output_dir, f"{args.dataset}_variants.json")
    process_with_jsonl_parallel(dataset, extract_path, extract_and_generate_variants, "Generating variants")
    
    # Step 2: Verify with Sampling (å¹¶è¡Œå¤„ç†å˜ä½“)
    print(f"\n[2/3] Verifying incomplete questions with sampling (n={args.max_attempts}, parallel)")
    dataset = read_json(extract_path)
    final_path = os.path.join(output_dir, f"{args.dataset}_final.json")
    process_with_jsonl_parallel(dataset, final_path, verify_incomplete_questions_with_sampling, "Verifying with sampling")
    
    # Filter
    print("\n[3/3] Filtering valid data")
    filter_valid_data(final_path)
    
    print("\nâœ“ Pipeline completed!")

if __name__ == "__main__":
    construction_workflow()