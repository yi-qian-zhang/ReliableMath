#!/usr/bin/env python3
"""
è¾“å…¥æ•°æ®:åŸå§‹æ•°å­¦é—®é¢˜ (question)+æ ‡å‡†ç­”æ¡ˆ (ground_truth)  +éš¾åº¦æ ‡ç­¾ (difficulty)

Step 1. GPT-4oæå–æ¡ä»¶å¹¶æ”¹å†™ (extract_and_generate_variants): 
   â†’ è®©LLMæå–è¯¥é—®é¢˜ä¸­çš„[å…³é”®æ¡ä»¶]ï¼Œæ”¹å†™å‰©ä½™å†…å®¹ä¸º{ç¼ºçœé—®é¢˜},[å…³é”®æ¡ä»¶]+{ç¼ºçœé—®é¢˜}ç»„æˆpairs
   â†“

ç”Ÿæˆå¤šä¸ª removal_variants (ç§»é™¤å˜ä½“)

Step 2.éªŒè¯ç¼ºçœé—®é¢˜ (verify_incomplete_questions_multi_attempt) 
   â†’ distill 7B\qwen3-8Bå›ç­”8æ¬¡ ï¼Œå›ç­”å¯¹äº†å°±é€šè¿‡ï¼Œå›ç­”é”™äº†å°±ç»§ç»­è¯•åˆ°8æ¬¡
   â†’ ç»™æ¨¡å‹ï¼šç¼ºçœé—®é¢˜(incomplete_question) + è¢«ç§»é™¤çš„[å…³é”®æ¡ä»¶] ï¼ˆremoved_conditionï¼‰

   â†’ è°ƒç”¨ Judgeï¼ˆgpt-4o-miniï¼‰ åˆ¤æ–­ç­‰ä»·æ€§ï¼š                

â€‹        ç­”æ¡ˆ = ground_truth â†’ ä¿ç•™ï¼ˆæ¡ä»¶å¿…è¦ï¼‰
â€‹        ç­”æ¡ˆ â‰  ground_truth â†’ ä¸¢å¼ƒï¼ˆæ¡ä»¶éå¿…è¦ï¼‰
   â†“
æœ€ç»ˆæ•°æ®é›†ï¼šåªåŒ…å«ç§»é™¤å…³é”®æ¡ä»¶åçš„æœ‰æ•ˆç¼ºçœé—®é¢˜:-------------------------æ¯ä¸ªéš¾åº¦500æ¡
"""
  
import os
import json
import time
import logging
import argparse
from openai import OpenAI
import random
from tqdm import tqdm
import tiktoken
import glob
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

parser = argparse.ArgumentParser(description="MIP Dataset Construction - 2 Steps")
parser.add_argument("--model", default="gpt-4o", help="Model for extraction/rewrite")
parser.add_argument("--verify_model", default="deepseek-r1-distill-qwen-7b", help="Model for verification")
parser.add_argument("--judge_model", default="gpt-4o-mini", help="Model for LLM-as-Judge")
parser.add_argument("--data_dir", default="data/solve", help="Input directory")
parser.add_argument("--output_dir", default="data/construct_mip_data", help="Output directory")
parser.add_argument("--prompt_dir", default="prompt/construct_mip_data", help="Prompt directory")
parser.add_argument("--dataset", default="polaris_easy_20", help="Dataset name")
parser.add_argument("--temperature", default=0.9, type=float, help="Temperature for verification")
parser.add_argument("--max_attempts", default=8, type=int, help="Max attempts for verification")
parser.add_argument("--test_mode", action='store_true', help="Test mode - process only first 5 items")
parser.add_argument("--force", action='store_true', help="Force reprocess all items")
args = parser.parse_args()

# Load API config
try:
    api_config_path = "data/api_keys.json"
    model_options = json.load(open(api_config_path, "r"))
except FileNotFoundError:
    logging.error(f"api_keys.json not found at {api_config_path}!")
    logging.error(f"Please make sure you run this script from ~/ReliableMath directory")
    exit(1)

# ============= Utility Functions =============

def read_json(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def write_json(filepath, data):
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def read_jsonl(filepath):
    data = []
    if not os.path.exists(filepath):
        return data
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except:
                    continue
    return data

def dump_jsonl(data, filepath, append=False):
    mode = 'a' if append else 'w'
    try:
        json_str = json.dumps(data, ensure_ascii=False)
    except:
        return False
    with open(filepath, mode, encoding='utf-8') as f:
        f.write(json_str + '\n')
        f.flush()
    return True

def count_tokens(text, model_name="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model_name)
        return len(encoding.encode(text))
    except:
        return len(text) // 4

def record_tokens(data, model_type, prompt_tokens, completion_tokens):
    """
    æ ¹æ®æ¨¡å‹ç±»å‹è®°å½• token ä½¿ç”¨é‡
    
    å‚æ•°ï¼š
        data: æ•°æ®å­—å…¸
        model_type: "gpt-4o" / "gpt-4o-mini" / "local"
        prompt_tokens: è¾“å…¥ token æ•°
        completion_tokens: è¾“å‡º token æ•°
    """
    # åˆå§‹åŒ–å­—æ®µ
    if "gpt4o_prompt_lengths" not in data:
        data["gpt4o_prompt_lengths"] = []
        data["gpt4o_completion_lengths"] = []
    if "gpt4o_mini_prompt_lengths" not in data:
        data["gpt4o_mini_prompt_lengths"] = []
        data["gpt4o_mini_completion_lengths"] = []
    if "local_prompt_lengths" not in data:
        data["local_prompt_lengths"] = []
        data["local_completion_lengths"] = []
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è®°å½•
    if model_type == "gpt-4o":
        data["gpt4o_prompt_lengths"].append(prompt_tokens)
        data["gpt4o_completion_lengths"].append(completion_tokens)
    elif model_type == "gpt-4o-mini":
        data["gpt4o_mini_prompt_lengths"].append(prompt_tokens)
        data["gpt4o_mini_completion_lengths"].append(completion_tokens)
    elif model_type == "local":
        data["local_prompt_lengths"].append(prompt_tokens)
        data["local_completion_lengths"].append(completion_tokens)

# ============= API Functions =============

def get_response_openai(input_prompt, persona="", model=None, temperature=0.0):
    """
    è°ƒç”¨ OpenAI-compatible API
    
    è¿”å›ï¼š
        (response_text, prompt_tokens, completion_tokens, model_type)
        model_type: "gpt-4o" / "gpt-4o-mini" / "local"
    """
    if model is None:
        model = args.model
    
    if model not in model_options:
        logging.error(f"Model {model} not found")
        return "", 0, 0, "unknown"
    
    model_name, key, url = random.choice(model_options[model])
    client = OpenAI(api_key=key, base_url=url)
    
    messages = []
    if persona:
        messages.append({"role": "system", "content": persona})
    messages.append({"role": "user", "content": input_prompt})
    
    prompt_text = (persona + "\n" if persona else "") + input_prompt
    prompt_tokens = count_tokens(prompt_text, model_name)
    
    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    is_local_model = "localhost" in url or "127.0.0.1" in url
    
    if is_local_model:
        model_type = "local"
    elif "gpt-4o-mini" in model_name.lower():
        model_type = "gpt-4o-mini"
    elif "gpt-4o" in model_name.lower():
        model_type = "gpt-4o"
    else:
        model_type = "gpt-4o"
    
    max_retries = 5
    for attempt in range(max_retries):
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=8192,
                stream=False
            )
            
            response_text = completion.choices[0].message.content
            
            try:
                prompt_tokens = completion.usage.prompt_tokens
                completion_tokens = completion.usage.completion_tokens
            except:
                if is_local_model:
                    logging.debug(f"Local model: estimating tokens")
                completion_tokens = count_tokens(response_text, model_name)
            
            return response_text, prompt_tokens, completion_tokens, model_type
            
        except Exception as e:
            logging.warning(f'API call failed (attempt {attempt+1}/{max_retries}): {e}')
            if attempt < max_retries - 1:
                wait_time = 3 if is_local_model else 10
                time.sleep(wait_time * (attempt + 1))
    
    return "", 0, 0, model_type

def parse_json_response(response, fallback=None):
    """ç®€åŒ–çš„ JSON è§£æ"""
    try:
        start = response.find('[')
        end = response.rfind(']') + 1
        if start >= 0 and end > start:
            json_str = response[start:end]
            return json.loads(json_str)
        
        start = response.find('{')
        end = response.rfind('}') + 1
        if start >= 0 and end > start:
            json_str = response[start:end]
            return json.loads(json_str)
            
    except Exception as e:
        logging.error(f"JSON parsing failed: {e}")
        logging.error(f"Full response: {response}")
    
    return fallback if fallback is not None else {}

# ============= Answer Processing =============

def extract_answer_tag(response):
    """ä»å“åº”ä¸­æå–ç­”æ¡ˆï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
    try:
        # æ–¹æ³• 1: ä¼˜å…ˆæŸ¥æ‰¾ <answer> æ ‡ç­¾
        start = response.find('<answer>')
        end = response.find('</answer>')
        
        if start >= 0 and end > start:
            answer = response[start + 8:end].strip()
            if '\\boxed{' in answer:
                boxed_match = re.search(r'\\boxed\{([^}]+)\}', answer)
                if boxed_match:
                    return boxed_match.group(1).strip()
            return answer
        
        # æ–¹æ³• 2: æŸ¥æ‰¾ $\boxed{...}$ æˆ– \boxed{...} æ ¼å¼
        boxed_pattern = r'\$?\\boxed\{([^}]+)\}\$?'
        boxed_matches = re.findall(boxed_pattern, response)
        
        if boxed_matches:
            answer = boxed_matches[-1].strip()
            answer = answer.replace('$', '').strip()
            return answer
        
        # æ–¹æ³• 3: æŸ¥æ‰¾å¸¸è§çš„ç­”æ¡ˆæ ‡è®°
        answer_patterns = [
            r'[Ff]inal [Aa]nswer:?\s*(.+?)(?:\n|$)',
            r'[Tt]he answer is:?\s*(.+?)(?:\n|$)',
            r'[Aa]nswer:?\s*(.+?)(?:\n|$)',
        ]
        
        for pattern in answer_patterns:
            match = re.search(pattern, response)
            if match:
                answer = match.group(1).strip()
                if '\\boxed{' in answer:
                    boxed_match = re.search(r'\\boxed\{([^}]+)\}', answer)
                    if boxed_match:
                        return boxed_match.group(1).strip()
                return answer
        
        return None
        
    except Exception as e:
        logging.error(f"Failed to extract answer: {e}")
        return None

def judge_answer_equivalence(question, model_answer, ground_truth):
    """ä½¿ç”¨ LLM-as-Judge åˆ¤æ–­ç­”æ¡ˆç­‰ä»·æ€§"""
    prompt_path = os.path.join(args.prompt_dir, "judge_equivalence.txt")
    
    if not os.path.exists(prompt_path):
        logging.error(f"Judge prompt not found: {prompt_path}")
        return False, 0, 0, "unknown"
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    
    input_prompt = prompt_template.format(
        question=question,
        model_answer=model_answer,
        ground_truth=ground_truth
    )
    
    response, prompt_tokens, completion_tokens, model_type = get_response_openai(
        input_prompt,
        persona="You are an expert mathematical equivalence judge.",
        model=args.judge_model,
        temperature=0.0
    )
    
    response_lower = response.strip().lower()
    
    if 'true' in response_lower and 'false' not in response_lower:
        result = True
    elif response_lower == 'true':
        result = True
    else:
        result = False
    
    return result, prompt_tokens, completion_tokens, model_type

# ============= Step 1: Extract and Generate Variants =============

def extract_and_generate_variants(data):
    """Step 1: ä¸€æ¬¡æ€§æå–æ¡ä»¶å¹¶ç”Ÿæˆæ‰€æœ‰ç§»é™¤å˜ä½“"""
    prompt_path = os.path.join(args.prompt_dir, "extract_and_remove.txt")
    
    if not os.path.exists(prompt_path):
        logging.error(f"Prompt file not found: {prompt_path}")
        data["removal_variants"] = []
        return data
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    
    input_prompt = prompt_template.format(
        original_question=data["question"],
        ground_truth=data.get("ground_truth", "")
    )
    
    response, prompt_tokens, completion_tokens, model_type = get_response_openai(
        input_prompt,
        persona="You are an expert at analyzing and rewriting mathematical problems.",
        model=args.model,
        temperature=0.0
    )
    
    # è®°å½• token
    record_tokens(data, model_type, prompt_tokens, completion_tokens)
    
    # Parse response - æœŸæœ›å¾—åˆ°ä¸€ä¸ªå˜ä½“åˆ—è¡¨
    parsed = parse_json_response(response, {"variants": []})
    
    # å¤„ç†ä¸¤ç§å¯èƒ½çš„ JSON æ ¼å¼
    if isinstance(parsed, list):
        variants_data = parsed
    else:
        variants_data = parsed.get("variants", [])
    
    removal_variants = []
    
    for i, variant_data in enumerate(variants_data):
        # æ¸…ç† incomplete_question
        incomplete_question = variant_data.get("incomplete_question", "").strip()
        
        # Remove common prefixes
        for prefix in ["Rewritten Problem:", "Incomplete Problem:", "Problem:", "**Problem:**"]:
            if prefix in incomplete_question:
                incomplete_question = incomplete_question.split(prefix)[-1].strip()
        
        incomplete_question = incomplete_question.replace("**", "").strip()
        
        # Remove quotes if present
        if incomplete_question.startswith('"') and incomplete_question.endswith('"'):
            incomplete_question = incomplete_question[1:-1].strip()
        
        variant = {
            "variant_id": f"{data['id']}_remove_{i}",
            "removed_condition_index": i,
            "removed_condition": variant_data.get("removed_condition", ""),
            "remaining_conditions": variant_data.get("remaining_conditions", []),
            "incomplete_question": incomplete_question
        }
        
        removal_variants.append(variant)
    
    data["removal_variants"] = removal_variants
    
    logging.info(f"ID {data['id']}: Generated {len(removal_variants)} removal variants")
    
    return data

# ============= Step 2: Verify with Multiple Attempts =============

def verify_incomplete_questions_multi_attempt(data):
    """Step 2: éªŒè¯"ç¼ºçœé—®é¢˜ + ç§»é™¤çš„æ¡ä»¶"èƒ½å¦è§£å‡º ground_truthï¼ˆæœ€å¤š 8 æ¬¡å°è¯•ï¼‰"""
    prompt_path = os.path.join(args.prompt_dir, "verify_with_condition.txt")
    
    if not os.path.exists(prompt_path):
        logging.error(f"Prompt file not found: {prompt_path}")
        return data
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    
    ground_truth = str(data.get("ground_truth", "")).strip()
    
    for variant in data.get("removal_variants", []):
        incomplete_question = variant["incomplete_question"]
        removed_condition = variant["removed_condition"]
        
        # è®°å½•æ‰€æœ‰å°è¯•
        all_attempts = []
        success_at_attempt = None
        is_valid = False
        
        # æœ€å¤šå°è¯• max_attempts æ¬¡
        for attempt_num in range(1, args.max_attempts + 1):
            input_prompt = prompt_template.format(
                incomplete_question=incomplete_question,
                removed_condition=removed_condition
            )
            
            response, prompt_tokens, completion_tokens, model_type = get_response_openai(
                input_prompt,
                persona="You are an expert mathematical problem solver.",
                model=args.verify_model,
                temperature=args.temperature
            )
            
            # è®°å½• token
            record_tokens(data, model_type, prompt_tokens, completion_tokens)
            
            # æå–ç­”æ¡ˆ
            model_answer = extract_answer_tag(response)
            
            # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
            if model_answer is None:
                is_correct = False
                judge_result = "no_answer_tag"
            else:
                is_correct, judge_prompt_tokens, judge_completion_tokens, judge_model_type = judge_answer_equivalence(
                    incomplete_question + " [With condition: " + removed_condition + "]",
                    model_answer,
                    ground_truth
                )
                judge_result = "equivalent" if is_correct else "not_equivalent"
                
                # è®°å½• judge token
                record_tokens(data, judge_model_type, judge_prompt_tokens, judge_completion_tokens)
            
            # è®°å½•æœ¬æ¬¡å°è¯•
            attempt_record = {
                "attempt": attempt_num,
                "model_answer": model_answer if model_answer else "N/A",
                "judge_result": judge_result,
                "is_correct": is_correct
            }
            all_attempts.append(attempt_record)
            
            # å¦‚æœç­”å¯¹äº†ï¼Œç«‹å³åœæ­¢
            if is_correct:
                success_at_attempt = attempt_num
                is_valid = True
                logging.info(f"ID {variant['variant_id']}: âœ“ VALID at attempt {attempt_num}/{args.max_attempts} (answer: {model_answer[:30]}...)")
                break
            else:
                logging.info(f"ID {variant['variant_id']}: Attempt {attempt_num}/{args.max_attempts} failed (answer: {model_answer[:30] if model_answer else 'N/A'}...)")
        
        # å¦‚æœ8æ¬¡éƒ½å¤±è´¥
        if not is_valid:
            logging.info(f"ID {variant['variant_id']}: âœ— INVALID - All {args.max_attempts} attempts failed")
        
        # ä¿å­˜éªŒè¯ç»“æœ
        variant["verification"] = {
            "total_attempts": len(all_attempts),
            "success_at_attempt": success_at_attempt,
            "is_valid": is_valid,
            "all_attempts": all_attempts,
            "ground_truth": ground_truth
        }
    
    return data

# ============= Pipeline Functions =============

def process_with_jsonl(dataset, output_path, process_func, desc):
    total_len = len(dataset)
    jsonl_path = output_path.replace('.json', '.jsonl')
    
    existing_data = []
    if os.path.exists(jsonl_path):
        existing_data = read_jsonl(jsonl_path)
        if existing_data:
            saved_ids = {item['id'] for item in existing_data}
            dataset = [item for item in dataset if item['id'] not in saved_ids]
            logging.info(f"{desc}: Continuing from {len(existing_data)} items")
    elif os.path.exists(output_path):
        try:
            existing_data = read_json(output_path)
            saved_ids = {item['id'] for item in existing_data}
            dataset = [item for item in dataset if item['id'] not in saved_ids]
        except:
            pass
    
    if not dataset:
        logging.info(f"{desc}: All items processed")
        return True
    
    with tqdm(total=len(dataset), desc=desc) as t:
        for data in dataset:
            try:
                processed_data = process_func(data)
                if processed_data:
                    t.update(1)
                    dump_jsonl(processed_data, jsonl_path, append=True)
            except Exception as e:
                logging.error(f"Error processing {data.get('id', 'unknown')}: {e}")
                import traceback
                traceback.print_exc()
                t.update(1)
                continue
    
    all_data = existing_data + read_jsonl(jsonl_path)[len(existing_data):]
    
    if all_data:
        write_json(output_path, all_data)
        if os.path.exists(jsonl_path):
            os.remove(jsonl_path)
    
    return len(all_data) == total_len

def filter_valid_data(final_path):
    """ç­›é€‰æœ‰æ•ˆçš„ç¼ºçœé—®é¢˜"""
    dataset = read_json(final_path)
    valid_data = []
    
    # åˆ†åˆ«ç»Ÿè®¡ä¸‰ç±»æ¨¡å‹çš„ token
    total_gpt4o_prompt = sum(sum(d.get("gpt4o_prompt_lengths", [])) for d in dataset)
    total_gpt4o_completion = sum(sum(d.get("gpt4o_completion_lengths", [])) for d in dataset)
    
    total_gpt4o_mini_prompt = sum(sum(d.get("gpt4o_mini_prompt_lengths", [])) for d in dataset)
    total_gpt4o_mini_completion = sum(sum(d.get("gpt4o_mini_completion_lengths", [])) for d in dataset)
    
    total_local_prompt = sum(sum(d.get("local_prompt_lengths", [])) for d in dataset)
    total_local_completion = sum(sum(d.get("local_completion_lengths", [])) for d in dataset)
    
    total_original = len(dataset)
    total_variants = 0
    valid_variants = 0
    
    # ç»Ÿè®¡å°è¯•æ¬¡æ•°åˆ†å¸ƒ
    attempt_distribution = {}
    
    for data in dataset:
        for variant in data.get("removal_variants", []):
            total_variants += 1
            
            verification = variant.get("verification", {})
            
            # åªä¿ç•™æœ‰æ•ˆçš„ pairï¼ˆåŠ å›æ¡ä»¶åèƒ½è§£å‡º ground_truthï¼‰
            if verification.get("is_valid", False):
                success_attempt = verification.get("success_at_attempt", 0)
                attempt_distribution[success_attempt] = attempt_distribution.get(success_attempt, 0) + 1
                
                valid_item = {
                    "id": variant["variant_id"],
                    "data_source": data.get("data_source", ""),
                    "difficulty": data.get("difficulty", ""),
                    "transformation_type": "condition_removal",
                    "original_question": data["question"],
                    "ground_truth": data.get("ground_truth", ""),
                    "removed_condition": variant["removed_condition"],
                    "removed_condition_index": variant["removed_condition_index"],
                    "remaining_conditions": variant["remaining_conditions"],
                    "incomplete_question": variant["incomplete_question"],
                    "verification": verification,
                    "original_id": data["id"]
                }
                valid_data.append(valid_item)
                valid_variants += 1
    
    output_path = final_path.replace("_final.json", "_valid.json")
    write_json(output_path, valid_data)
    
    # Statistics
    print("\n" + "="*70)
    print("MISSING INFORMATION PROBLEM (MIP) DATASET STATISTICS")
    print("="*70)
    print(f"Original problems: {total_original}")
    print(f"\nTotal removal variants generated: {total_variants}")
    print(f"Valid removal variants (condition necessary): {valid_variants}")
    if total_variants > 0:
        print(f"Success rate: {valid_variants / total_variants * 100:.2f}%")
    
    print(f"\nAttempt Distribution (when successful):")
    for attempt in sorted(attempt_distribution.keys()):
        count = attempt_distribution[attempt]
        print(f"  Attempt {attempt}: {count} variants ({count/valid_variants*100:.1f}%)")
    
    # å•ä»·ï¼ˆæ¯ 1M tokensï¼‰
    gpt4o_prompt_rate = 2.5
    gpt4o_completion_rate = 10.0
    gpt4o_mini_prompt_rate = 0.15
    gpt4o_mini_completion_rate = 0.6

    # GPT-4o Token ç»Ÿè®¡
    print(f"\nğŸ’° GPT-4o Token Usage:")
    print(f"  Prompt: {total_gpt4o_prompt:,}")
    print(f"  Completion: {total_gpt4o_completion:,}")
    print(
        f"  Cost = {total_gpt4o_prompt}/1e6*{gpt4o_prompt_rate} "
        f"+ {total_gpt4o_completion}/1e6*{gpt4o_completion_rate} "
        f"= ${total_gpt4o_prompt/1e6*gpt4o_prompt_rate + total_gpt4o_completion/1e6*gpt4o_completion_rate:.6f}"
    )

    # GPT-4o-mini Token ç»Ÿè®¡
    print(f"\nğŸ’° GPT-4o-mini Token Usage:")
    print(f"  Prompt: {total_gpt4o_mini_prompt:,}")
    print(f"  Completion: {total_gpt4o_mini_completion:,}")
    print(
        f"  Cost = {total_gpt4o_mini_prompt}/1e6*{gpt4o_mini_prompt_rate} "
        f"+ {total_gpt4o_mini_completion}/1e6*{gpt4o_mini_completion_rate} "
        f"= ${total_gpt4o_mini_prompt/1e6*gpt4o_mini_prompt_rate + total_gpt4o_mini_completion/1e6*gpt4o_mini_completion_rate:.6f}"
    )

    
    # æœ¬åœ°æ¨¡å‹ Token ç»Ÿè®¡
    print(f"\nğŸ–¥ï¸  Local Model Token Usage:")
    print(f"  Prompt: {total_local_prompt:,}")
    print(f"  Completion: {total_local_completion:,}")
    
    print(f"\nOutput: {output_path}")
    print("="*70)

# ============= Main Workflow =============

def construction_workflow():
    # ç›´æ¥ä½¿ç”¨ args ä¸­çš„è·¯å¾„ï¼ˆç›¸å¯¹äº ~/ReliableMathï¼‰
    input_path = os.path.join(args.data_dir, f"{args.dataset}.json")
    output_dir = args.output_dir
    
    if not os.path.exists(input_path):
        logging.error(f"Input not found: {input_path}")
        logging.error(f"Current working directory: {os.getcwd()}")
        logging.error(f"Please make sure you run this script from ~/ReliableMath directory")
        return
    
    dataset = read_json(input_path)
    
    if args.test_mode:
        dataset = dataset[:5]
        logging.info("TEST MODE: First 5 items")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Force cleanup
    if args.force:
        logging.info("Force mode: Cleaning up existing intermediate files...")
        for pattern in [f"{args.dataset}_*.json", f"{args.dataset}_*.jsonl"]:
            for file in glob.glob(os.path.join(output_dir, pattern)):
                try:
                    os.remove(file)
                    logging.info(f"Removed: {file}")
                except Exception as e:
                    logging.warning(f"Could not remove {file}: {e}")
        logging.info("Cleanup completed.")
    
    print("="*70)
    print("MISSING INFORMATION PROBLEM (MIP) CONSTRUCTION PIPELINE - 2 STEPS")
    print("="*70)
    print(f"Working directory: {os.getcwd()}")
    print(f"Input: {input_path}")
    print(f"Output: {output_dir}")
    print(f"Prompt: {args.prompt_dir}")
    print(f"Model (extract/rewrite): {args.model}")
    print(f"Model (verify): {args.verify_model}")
    print(f"Model (judge): {args.judge_model}")
    print(f"Temperature: {args.temperature}")
    print(f"Max attempts: {args.max_attempts}")
    print(f"Items: {len(dataset)}")
    if args.force:
        print(f"Mode: FORCE (reprocessing all)")
    print("="*70)
    
    # Step 1: Extract and Generate Variants
    print("\n[1/2] Extracting conditions and generating removal variants")
    extract_path = os.path.join(output_dir, f"{args.dataset}_variants.json")
    process_with_jsonl(dataset, extract_path, extract_and_generate_variants, "Generating variants")
    
    # Step 2: Verify with Multiple Attempts
    print(f"\n[2/2] Verifying incomplete questions (max {args.max_attempts} attempts)")
    dataset = read_json(extract_path)
    final_path = os.path.join(output_dir, f"{args.dataset}_final.json")
    process_with_jsonl(dataset, final_path, verify_incomplete_questions_multi_attempt, "Verifying")
    
    # Filter
    print("\n[3/2] Filtering valid data")
    filter_valid_data(final_path)
    
    print("\nâœ“ Pipeline completed!")

if __name__ == "__main__":
    construction_workflow()